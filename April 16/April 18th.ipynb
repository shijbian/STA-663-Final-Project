{
 "metadata": {
  "name": "",
  "signature": "sha256:a9e01743c8a6c47202ccf5d2b01e5f620e7836318fe0dbdc2550c352028f4189"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Bayesian Hierarchical Clustering \n",
      "============="
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Abstract\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Outline"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Background\n",
      "\n",
      "State the research paper you are using. Describe the concept of the algorithm and why it is interesting and/or useful. If appropriate, describe the mathematical basis of the algorithm. Some potential topics for the backgorund include:\n",
      "\n",
      "- What problem does it address? \n",
      "- What are known and possible applications of the algorithm? \n",
      "- What are its advantages and disadvantages relative to other algorithms?\n",
      "- How will you use it in your research?\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Basic Algorithm\n",
      "\n",
      "\n",
      "**input** data $\\mathcal{D} = {x^{(1)}, x^{(2)}, \\ldots, x^{(n)}}$, model $p(x|\\theta)$, prior $p(\\theta|\\beta)$\n",
      "\n",
      "**initialize: ** number of clusters $\\mathcal{c}=\\mathcal{n}$ for $i = 1, \\ldots, n$\n",
      "\n",
      "**while** $\\mathcal{c}>1$ **do**:\n",
      "\n",
      "+ Find the pair $\\mathcal{D}_i$ and $\\mathcal{D}_j$ with the highest probability of the merged hypothesis:\n",
      " $$\\mathcal{r}_k = \\frac{\\pi_k p(\\mathcal{D}_k|\\mathcal{H}_1^k)}{p(\\mathcal{D}_k|\\mathcal{T}_k)}$$\n",
      "+ Merge $\\mathcal{D}_k \\leftarrow \\mathcal{D}_i \\cup \\mathcal{D}_j$, $\\mathcal{T}_k \\leftarrow (\\mathcal{T}_i, \\mathcal{T}_j)$\n",
      "+ Delete $\\mathcal{D}_i$ and $\\mathcal{D}_j$, $c \\leftarrow c-1$\n",
      "\n",
      "**end while**\n",
      "\n",
      "**output: ** Bayesian mixture model where each tree node is a mixture component\n",
      "The tree can be cut at points where $\\mathcal{r}_k<0.5$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Case Study I: Gaussion Distribution with Unknown Mean and Precision\n",
      "\n",
      "Assume each observation $x^{(i)}$ consists of $d$ variables, i.e.$x^{(i)}=(x_1^{(i)},\\ldots,x_d^{(i)})$, we need make the assumptions below to accomodate the algorithm:\n",
      "\n",
      "+ the dataset is normalized, i.e. it has mean sero and a unit variance;\n",
      "+ for each observation $x^{(i)}$,its variables ${x_j^{(i)}}_{j=1}^d$ are independnt and generated from different Gaussian distributions.\n",
      "+ the realizations of each variable j, ${x_j^{(i)}}_{j=1}^{n_k}$ in cluster $\\mathcal{D}_k$ are independent and identically distributed and drawn from Gaussian distribution with unknown mean $\\mu_j$ and precision $\\sigma_j^2$, and the prior on $(\\mu_j,\\sigma_j^{-2})$ is a normal-gamma distribution with hyperparameter $\\mu_0,\\sigma_0,\\beta_0,\\kappa$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import scipy.stats as stats\n",
      "from IPython.core.pylabtools import figsize\n",
      "import numpy as np\n",
      "figsize(12.5, 4)\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "\n",
      "%matplotlib inline\n",
      "import scipy.stats as stats\n",
      "from IPython.core.pylabtools import figsize\n",
      "import numpy as np\n",
      "figsize(12.5, 4)\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "\n",
      "import itertools\n",
      "\n",
      "import numpy as np\n",
      "from scipy import linalg\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib as mpl\n",
      "\n",
      "from sklearn import mixture"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Simulate data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mean = (1,2)\n",
      "cov = [[1,0],[0,1]]\n",
      "dat = np.random.multivariate_normal(mean,cov,100)\n",
      "print dat.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(100, 2)\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def likelihood (data, d):\n",
      "    likelihood = 1\n",
      "    for j in range(0,d):\n",
      "        chi_0 = 1\n",
      "        n_k = nth cluster's size\n",
      "        chi_nk = chi_0 + n_k\n",
      "        \n",
      "        lambda_0 = 1\n",
      "        lambda_nk = lambda_0 + n_k*1.0/2\n",
      "        x_j_bar\n",
      "        \n",
      "        beta_0 = 1\n",
      "        beta_nk = beta_0 + 0.5*(variance of jth cluster + chi_0*n_k + mean of jth cluster)\n",
      "        \n",
      "        partialLik = (gamma(lambda_nk)*1.0/gamma(lambda_0))*(pow(beta_0,lambda_0)/pow(beta_nk,lambda_nk))*pow((chi_0*1.0/chi_nk),0.5)*pow(2*pi,-n_k*1.0/2)\n",
      "        likelihood = likelihood * partialLik\n",
      "    return likelihood\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def hcluster(data, features, distance=likelihood):\n",
      "    clust = data.shape[0]\n",
      "    while len(clust)>1:\n",
      "        lowestpair=(0,1)\n",
      "        closest=distance(clust[0].vec,clust[1].vec)\n",
      "\n",
      "        # loop through every pair looking for the smallest distance\n",
      "        for i in range(len(clust)):\n",
      "            for j in range(i+1,len(clust)):\n",
      "                # distances is the cache of distance calculations\n",
      "                if (clust[i].id,clust[j].id) not in distances: \n",
      "                    distances[(clust[i].id,clust[j].id)]=distance(clust[i].vec,clust[j].vec)\n",
      "\n",
      "                d=distances[(clust[i].id,clust[j].id)]\n",
      "\n",
      "                if d < 0.5:\n",
      "                    closest=d\n",
      "                    lowestpair=(i,j)\n",
      "\n",
      "        # calculate the average of the two clusters\n",
      "        mergevec=[(clust[lowestpair[0]].vec[i]+clust[lowestpair[1]].vec[i])/2.0 \\\n",
      "            for i in range(len(clust[0].vec))]\n",
      "\n",
      "        # create the new cluster\n",
      "        newcluster=cluster_node(array(mergevec),left=clust[lowestpair[0]],\n",
      "                             right=clust[lowestpair[1]],\n",
      "                             distance=closest,id=currentclustid)\n",
      "\n",
      "        # cluster ids that weren't in the original set are negative\n",
      "        currentclustid-=1\n",
      "        del clust[lowestpair[1]]\n",
      "        del clust[lowestpair[0]]\n",
      "        clust.append(newcluster)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}