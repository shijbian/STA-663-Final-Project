{
 "metadata": {
  "name": "",
  "signature": "sha256:696fa071264162d0d2b867310203cff790ba3dd783240b1a7ec9a28d807d17a2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Reference: http://stats.stackexchange.com/questions/72196/pymc-for-nonparametric-clustering-dirichlet-process-to-estimate-gaussian-mixtur"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pymc\n",
      "import numpy as np\n",
      "\n",
      "### Data generation\n",
      "\n",
      "# Means and standard deviations of the Gaussian mixture model. The inference\n",
      "# engine doesn't know these.\n",
      "means = [0, 4.0]\n",
      "stdevs = [1, 2.0]\n",
      "\n",
      "# Rather than randomizing between the mixands, just specify how many\n",
      "# to draw from each. This makes it really easy to know which draws\n",
      "# came from which mixands (the first N1 from the first, the rest from\n",
      "# the secon). The inference engine doesn't know about N1 and N2, only Ndata\n",
      "N1 = 10\n",
      "N2 = 40\n",
      "Ndata = N1+N2\n",
      "\n",
      "# Seed both the data generator RNG  as well as the global seed (for PyMC)\n",
      "RNGseed = 123\n",
      "np.random.seed(RNGseed)\n",
      "\n",
      "def generate_data(draws_per_mixand):\n",
      "    \"\"\"Draw samples from a two-element Gaussian mixture reproducibly.\n",
      "\n",
      "    Input sequence indicates the number of draws from each mixand. Resulting\n",
      "    draws are concantenated together.\n",
      "\n",
      "    \"\"\"\n",
      "    RNG = np.random.RandomState(RNGseed)\n",
      "    values = np.hstack([RNG.normal(means[i], stdevs[i], ndraws)\n",
      "                        for (i,ndraws) in enumerate(draws_per_mixand)])\n",
      "    return values\n",
      "\n",
      "observed_data = generate_data([N1, N2])\n",
      "\n",
      "\n",
      "### PyMC model setup, step 1: the Dirichlet process and stick-breaking\n",
      "\n",
      "# Truncation level of the Dirichlet process\n",
      "Ndp = 50\n",
      "\n",
      "concinit = 5.0\n",
      "conclo = 0.3\n",
      "conchi = 100.0\n",
      "concentration = pymc.Uniform('concentration', lower=conclo, upper=conchi,\n",
      "                             value=concinit)\n",
      "\n",
      "# The stick-breaking construction: requires Ndp beta draws dependent on the\n",
      "# concentration, before the probability mass function is actually constructed.\n",
      "betas = pymc.Beta('betas', alpha=1, beta=concentration, size=Ndp)\n",
      "\n",
      "@pymc.deterministic\n",
      "def pmf(betas=betas):\n",
      "    \"Construct a probability mass function for the truncated Dirichlet process\"\n",
      "    # prod = lambda x: np.exp(np.sum(np.log(x))) # Slow but more accurate(?)\n",
      "    prod = np.prod\n",
      "    value = map(lambda (i,u): u * prod(1.0 - betas[:i]), enumerate(betas))\n",
      "    value[-1] = 1.0 - sum(value[:-1]) # force value to sum to 1\n",
      "    return value\n",
      "\n",
      "# The cluster assignments: each data point's estimated cluster ID.\n",
      "# Remove idinit to allow clusterid to be randomly initialized:\n",
      "idinit = np.zeros(Ndata, dtype=np.int64)\n",
      "clusterid = pymc.Categorical('clusterid', p=pmf, size=Ndata, value=idinit)\n",
      "\n",
      "### PyMC model setup, step 2: clusters' means and stdevs\n",
      "\n",
      "# An individual data sample is drawn from a Gaussian, whose mean and stdev is\n",
      "# what we're seeking.\n",
      "\n",
      "# Hyperprior on clusters' means\n",
      "mu0_mean = 0.0\n",
      "mu0_std = 50.0\n",
      "mu0_prec = 1.0/mu0_std**2\n",
      "mu0_init = np.zeros(Ndp)\n",
      "clustermean = pymc.Normal('clustermean', mu=mu0_mean, tau=mu0_prec,\n",
      "                          size=Ndp, value=mu0_init)\n",
      "\n",
      "# The cluster's stdev\n",
      "clustersig_lo = 0.0\n",
      "clustersig_hi = 100.0\n",
      "clustersig_init = 50*np.ones(Ndp) # Again, don't really care?\n",
      "clustersig = pymc.Uniform('clustersig', lower=clustersig_lo,\n",
      "                          upper=clustersig_hi, size=Ndp, value=clustersig_init)\n",
      "clusterprec = clustersig ** -2\n",
      "\n",
      "### PyMC model setup, step 3: data\n",
      "\n",
      "# So now we have means and stdevs for each of the Ndp clusters. We also have a\n",
      "# probability mass function over all clusters, and a cluster ID indicating which\n",
      "# cluster a particular data sample belongs to.\n",
      "\n",
      "@pymc.deterministic\n",
      "def data_cluster_mean(clusterid=clusterid, clustermean=clustermean):\n",
      "    \"Converts Ndata cluster IDs and Ndp cluster means to Ndata means.\"\n",
      "    return clustermean[clusterid]\n",
      "\n",
      "@pymc.deterministic\n",
      "def data_cluster_prec(clusterid=clusterid, clusterprec=clusterprec):\n",
      "    \"Converts Ndata cluster IDs and Ndp cluster precs to Ndata precs.\"\n",
      "    return clusterprec[clusterid]\n",
      "\n",
      "data = pymc.Normal('data', mu=data_cluster_mean, tau=data_cluster_prec,\n",
      "                   observed=True, value=observed_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ImportError",
       "evalue": "No module named pymc",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-1-143fa101549d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpymc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m### Data generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mImportError\u001b[0m: No module named pymc"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**input** data $\\mathcal{D} = {x^{(1)}, x^{(2)}, \\ldots, x^{(n)}}$, model $p(x|\\theta)$, prior $p(\\theta|\\beta)$\n",
      "\n",
      "**initialize: ** number of clusters $\\mathcal{c}=\\mathcal{n}$ for $i = 1, \\ldots, n$\n",
      "\n",
      "**while** $\\mathcal{c}>1$ **do**:\n",
      "\n",
      "+ Find the pair $\\mathcal{D}_i$ and $\\mathcal{D}_j$ with the highest probability of the merged hypothesis:\n",
      " $$\\mathcal{r}_k = \\frac{\\pi_k p(\\mathcal{D}_k|\\mathcal{H}_1^k)}{p(\\mathcal{D}_k|\\mathcal{T}_k)}$$\n",
      "+ Merge $\\mathcal{D}_k \\leftarrow \\mathcal{D}_i \\cup \\mathcal{D}_j$, $\\mathcal{T}_k \\leftarrow (\\mathcal{T}_i, \\mathcal{T}_j)$\n",
      "+ Delete $\\mathcal{D}_i$ and $\\mathcal{D}_j$, $c \\leftarrow c-1$\n",
      "\n",
      "**end while**\n",
      "\n",
      "**output: ** Bayesian mixture model where each tree node is a mixture component\n",
      "The tree can be cut at points where $\\mathcal{r}_k<0.5$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}