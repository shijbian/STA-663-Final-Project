{
 "metadata": {
  "name": "",
  "signature": "sha256:a046cce76a9aa49d30c844bf7e1ff2c65c6c413491ba395e843b7a6f07a2b9c8"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Bayesian Hierarchical Clustering \n",
      "=============\n",
      "\n",
      "#### STA 663 Computational Statistics in Python Final Project\n",
      "\n",
      "#### Shijia Bian\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Abstract\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Outline  \n",
      "\n",
      "+ #### Background  \n",
      "\n",
      "+ #### Traditional Hierarchical Clustering  \n",
      "\n",
      "+ #### Algorithm Debrief\n",
      "\n",
      "+ #### Case Study\n",
      "\n",
      "+ #### Limitation of Implemented Algorithm\n",
      "\n",
      "+ #### Comparison to Traditional Clustering Methods\n",
      "\n",
      "+ #### Further Improvement and Pseudo Code\n",
      "\n",
      "+ #### Conclusion \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Background\n",
      "\n",
      "State the research paper you are using. Describe the concept of the algorithm and why it is interesting and/or useful. If appropriate, describe the mathematical basis of the algorithm. Some potential topics for the backgorund include:\n",
      "\n",
      "- What problem does it address? \n",
      "- What are known and possible applications of the algorithm? \n",
      "- What are its advantages and disadvantages relative to other algorithms?\n",
      "- How will you use it in your research?\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Traditional Hierarchical Clustering  \n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Algorithm Debrief\n",
      "\n",
      "#### Diagram\n",
      "![alt text](https://raw.githubusercontent.com/shijbian/STA-663-Final-Project/master/April%2023/nodeGraph.png)\n",
      "\n",
      "#### Notation  \n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\mathcal{D} = \\{\\bf{x}^{(1)}, \\ldots, \\bf{x}^{(n)} \\}$: entire data set.\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $T_i$: subtree $i$.  \n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\mathcal{D}_i$: data set in subtree $i$.\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $T_i \\cup T_j \\Rightarrow T_k \\rightarrow \\mathcal{D}_i \\cup \\mathcal{D}_j$: tree $T_i$ and tree $T_j$ merge to become a new tree $T_k$. \n",
      "\n",
      "#### Hypothesis Testing\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Null Hypothesis $\\mathcal{H}_1^k$**: all data in $\\mathcal{D}_k$ are i.i.d generated from the same probabilistic model $P(\\bf{x}|\\theta)$.  \n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Alternative Hypothesis $\\mathcal{H}_2^k$**: data in $\\mathcal{D}_k$ are from two or more clusters.\n",
      "\n",
      "#### Marginal Likelihood for the Hypothesis\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Likelihood for Null Hypothesis:** data at tree $\\mathcal{D}_k$ is generated from the same cluster\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  $P(\\mathcal{D}_k|H_1^k)= \\int P(\\mathcal{D}_k|\\theta) P(\\theta|\\beta) \\mathcal{d}\\theta$\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Likelihood for Alternative Hypothesis:** data at tree $\\mathcal{D}_k$ is generated from different cluster\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  $P(\\mathcal{D}_k|H_2^k)=P(\\mathcal{D}_i|T_i)P(\\mathcal{D}_j|T_j)$  \n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  **Marginal Probability of the Data in Tree $T_k$:**\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  $P(\\mathcal{D}_k|T_k)=\\pi_kP(\\mathcal{D}_k|H^k_1)+(1-\\pi_k)P(\\mathcal{D}_k|H^k_2)$  \n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  $\\pi_k = P(H^k_1)$\n",
      "\n",
      "#### Posterial Likelihood for the Hypothesis\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **$r_k=\\frac{\\pi_kp(\\mathcal{D}_k)|\\mathcal{H}_1^k}{\\pi_kP(\\mathcal{D}_k|H^k_1)+(1-\\pi_k)P(\\mathcal{D}_k|H^k_2)}$**\n",
      "\n",
      "#### Pseudocode for General Implementation\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **input** data $\\mathcal{D} = {x^{(1)}, x^{(2)}, \\ldots, x^{(n)}}$, model $p(x|\\theta)$, prior $p(\\theta|\\beta)$\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **initialize: ** number of clusters $\\mathcal{c}=\\mathcal{n}$ for $i = 1, \\ldots, n$\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **while** $\\mathcal{c}>1$ **do**:\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  Find the pair $\\mathcal{D}_i$ and $\\mathcal{D}_j$ with the highest probability of the merged hypothesis:\n",
      " $$\\mathcal{r}_k = \\frac{\\pi_k p(\\mathcal{D}_k|\\mathcal{H}_1^k)}{p(\\mathcal{D}_k|\\mathcal{T}_k)}$$\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Merge $\\mathcal{D}_k \\leftarrow \\mathcal{D}_i \\cup \\mathcal{D}_j$, $\\mathcal{T}_k \\leftarrow (\\mathcal{T}_i, \\mathcal{T}_j)$  \n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Delete $\\mathcal{D}_i$ and $\\mathcal{D}_j$, $c \\leftarrow c-1$\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **end while**\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **output: ** Bayesian mixture model where each tree node is a mixture component  \n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The tree can be cut at points where $\\mathcal{r}_k<0.5$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Case Study I: Gaussion Distribution with Unknown Mean and Known Precision\n",
      "\n",
      "In this case study, assume that the data are from two normal distributions with unknown mean and known variance.  \n",
      "\n",
      "$$X_1 \\sim \\mathcal{N}(\\mu_1,1) \\text{ and } X_2 \\sim \\mathcal{N}(\\mu_2,1)$$\n",
      "\n",
      "The prior for $\\mu_1$ and $\\mu_2$ is:  \n",
      "\n",
      "$$\\mu_1,\\mu_2 \\sim N(1,1)$$\n",
      "\n",
      "This setup can accomodate the algorithm from the perspectives below:\n",
      "\n",
      "+ the dataset can be normalized through the formula for the standard normal, i.e. it has mean zero and a unit variance;\n",
      "+ each observation $x^{(i)}$ is independnt and generated from different Gaussian distributions.\n",
      "+ the realizations of each variable j, ${x_j^{(i)}}_{j=1}^{n_k}$ in cluster $\\mathcal{D}_k$ are independent and identically distributed and drawn from Gaussian distribution with unknown mean $\\mu_j$ and precision $\\sigma_j^2$, and the prior on $(\\mu_j,\\sigma_j^{-2})$ is a normal-gamma distribution with hyperparameter $\\mu_0,\\sigma_0,\\beta_0,\\kappa$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "$$P(\\mathcal{D}|H_1)= \\int f(x|\\mu)f(\\mu) \\mathcal{d}\\mu = \\int_{-\\infty}^{\\infty} \\bigg[\\frac{1}{\\sigma_0 \\sqrt{2\\pi}}exp(-\\frac{(\\mu-\\mu_0)^2}{2\\sigma_0^2})\\bigg]\\bigg[(\\frac{1}{\\sigma \\sqrt{s\\pi}})^n exp(-\\frac{n(\\bar{x}-\\mu)^2}{2\\sigma^2})   \\bigg]\\mathcal{d}\\mu$$\n",
      "\n",
      "\n",
      "$$P(\\mathcal{D}|H_2)= \\int f(x|\\mu)f(\\mu) \\mathcal{d}\\mu = \\int_{-\\infty}^{\\infty} \\bigg[\\frac{1}{\\sigma_0 \\sqrt{2\\pi}}exp(-\\frac{(\\mu-\\mu_0)^2}{2\\sigma_0^2})\\bigg]\\bigg[(\\frac{1}{\\sigma \\sqrt{s\\pi}})^n exp(-\\frac{n(\\bar{x}-\\mu)^2}{2\\sigma^2})   \\bigg]\\mathcal{d}\\mu$$\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import scipy.stats as stats\n",
      "from IPython.core.pylabtools import figsize\n",
      "import numpy as np\n",
      "figsize(12.5, 4)\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "\n",
      "%matplotlib inline\n",
      "import scipy.stats as stats\n",
      "from IPython.core.pylabtools import figsize\n",
      "import numpy as np\n",
      "figsize(12.5, 4)\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "\n",
      "import itertools\n",
      "\n",
      "import numpy as np\n",
      "from scipy import linalg\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib as mpl\n",
      "\n",
      "from sklearn import mixture\n",
      "\n",
      "import operator"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Revised Algorithm \n",
      "\n",
      "+ **Step 1: Data Simulation:** \n",
      "simulate n data from two normal distribution: N(0, 1) and N(20, 1). This two cluster are distinguishable from each other. The default setup is 50% from the cluster 0 and 50% from the cluster 1;\n",
      "\n",
      "+ **Step 2: Initialization:** \n",
      "Initialize the data by extracting one pair of the data from each of the 2 clusters.\n",
      "\n",
      "+ **Step 3: Actual Algorithm:** \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Step 1: Data Simulation\n",
      "\n",
      "Data are simulated from two normal distribution:\n",
      "\n",
      "$$X_1 \\sim \\mathcal{N}(\\mu_1,1) \\text{ and } X_2 \\sim \\mathcal{N}(\\mu_2,1)$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mean = (1,20)\n",
      "cov = [[1,0],[0,1]]\n",
      "n=10\n",
      "dat = np.random.multivariate_normal(mean,cov,n)\n",
      "print dat"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[  1.36682543  20.26957114]\n",
        " [  1.28000783  19.7095942 ]\n",
        " [  0.10477382  21.17552875]\n",
        " [  1.19397459  18.03301193]\n",
        " [  0.70304333  18.92806637]\n",
        " [ -0.45101687  20.80194742]\n",
        " [  0.38752295  19.36558242]\n",
        " [  0.27967941  18.48118685]\n",
        " [  0.66116365  21.26335791]\n",
        " [  2.72518129  19.47912943]]\n"
       ]
      }
     ],
     "prompt_number": 69
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# reshape the dataset to be a 20*1 \n",
      "data = dat.reshape((n*2, 1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 70
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can visualize the plots as below:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.hist(data, bins=20, color=\"k\", histtype=\"stepfilled\", alpha=0.8)\n",
      "plt.title(\"Histogram of the dataset\")\n",
      "plt.ylim([0, None])\n",
      "print data[:10], \"...\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[  1.36682543]\n",
        " [ 20.26957114]\n",
        " [  1.28000783]\n",
        " [ 19.7095942 ]\n",
        " [  0.10477382]\n",
        " [ 21.17552875]\n",
        " [  1.19397459]\n",
        " [ 18.03301193]\n",
        " [  0.70304333]\n",
        " [ 18.92806637]] ...\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAtYAAAEKCAYAAAA2BBIPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBgCAAQRrAAAYQLAGAIABBGsAABjg/wO7Bc4rPAfOvwAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7f30dee3c310>"
       ]
      }
     ],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "\n",
      "df = pd.DataFrame({'data': data.reshape(-1)})\n",
      "df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>data</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0 </th>\n",
        "      <td>  1.366825</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1 </th>\n",
        "      <td> 20.269571</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2 </th>\n",
        "      <td>  1.280008</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3 </th>\n",
        "      <td> 19.709594</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4 </th>\n",
        "      <td>  0.104774</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5 </th>\n",
        "      <td> 21.175529</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6 </th>\n",
        "      <td>  1.193975</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7 </th>\n",
        "      <td> 18.033012</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8 </th>\n",
        "      <td>  0.703043</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9 </th>\n",
        "      <td> 18.928066</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>10</th>\n",
        "      <td> -0.451017</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>11</th>\n",
        "      <td> 20.801947</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>12</th>\n",
        "      <td>  0.387523</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>13</th>\n",
        "      <td> 19.365582</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>14</th>\n",
        "      <td>  0.279679</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>15</th>\n",
        "      <td> 18.481187</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>16</th>\n",
        "      <td>  0.661164</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>17</th>\n",
        "      <td> 21.263358</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>18</th>\n",
        "      <td>  2.725181</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>19</th>\n",
        "      <td> 19.479129</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 72,
       "text": [
        "         data\n",
        "0    1.366825\n",
        "1   20.269571\n",
        "2    1.280008\n",
        "3   19.709594\n",
        "4    0.104774\n",
        "5   21.175529\n",
        "6    1.193975\n",
        "7   18.033012\n",
        "8    0.703043\n",
        "9   18.928066\n",
        "10  -0.451017\n",
        "11  20.801947\n",
        "12   0.387523\n",
        "13  19.365582\n",
        "14   0.279679\n",
        "15  18.481187\n",
        "16   0.661164\n",
        "17  21.263358\n",
        "18   2.725181\n",
        "19  19.479129"
       ]
      }
     ],
     "prompt_number": 72
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given that our model is a DPM we can\n",
      "compute\n",
      "\u2013 \ud835\udc43 \ud835\udc37\ud835\udc58 \ud835\udc3b1\n",
      "\ud835\udc58\n",
      "- data at tree \ud835\udc47\ud835\udc58 was generated from\n",
      "the same cluster.\n",
      "\u2013 \ud835\udc43 \ud835\udc37\ud835\udc58 \ud835\udc3b1\n",
      "\ud835\udc58 = \u222b \ud835\udc5d \ud835\udc37\ud835\udc58 \ud835\udf03 \ud835\udc5d \ud835\udf03 \ud835\udefd \ud835\udc51\ud835\udf03\n",
      "\u2013 Easy to compute if the model has conjugacy\n",
      "\n",
      "\n",
      "\ud835\udc43 \ud835\udc37\ud835\udc58 \ud835\udc3b2\n",
      "\ud835\udc58\n",
      "- \ud835\udc37\ud835\udc58was generated from two or\n",
      "more components defining partitions\n",
      "consistent with trees \ud835\udc47\ud835\udc56 \ud835\udc4e\ud835\udc5b\ud835\udc51 \ud835\udc47\ud835\udc57\n",
      "\u2013 \ud835\udc43 \ud835\udc37\ud835\udc58 \ud835\udc3b2\n",
      "\ud835\udc58 = \ud835\udc43 \ud835\udc37\ud835\udc56 \ud835\udc47\ud835\udc56 \ud835\udc43 \ud835\udc37\ud835\udc57 \ud835\udc47\ud835\udc57)\n",
      "\u2013 \ud835\udc43 \ud835\udc37\ud835\udc58 \ud835\udc47\ud835\udc58 = \ud835\udf0b\ud835\udc58\ud835\udc5d \ud835\udc37\ud835\udc58 \ud835\udc3b\ud835\udc58\n",
      "1 + 1 \u2212 \ud835\udf0b\ud835\udc58 \ud835\udc43 \ud835\udc37\ud835\udc58 \ud835\udc3b2\n",
      "\ud835\udc58\n",
      "\u2022 \ud835\udf0b\ud835\udc58 = \ud835\udc5d(\ud835\udc3b\ud835\udc58\n",
      "1\n",
      ")"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![alt text](https://raw.githubusercontent.com/shijbian/STA-663-Final-Project/master/April%2023/nodeGraph.png)\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Self-design Algorithm\n",
      "\n",
      "+ **Select two pairs of points. Each of them are located separately in the two clusters.**\n",
      "\n",
      "+ **Define the function D_k**\n",
      "\n",
      "+ **Define the function hcluster:**\n",
      "    \n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; t_clust_0: initialized $T_i$  \n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; t_clust_1: initialized $T_j$  \n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This function will take into the parameters below:  \n",
      "\n",
      "+ data: the entire data set;\n",
      "+ n: the number of data;\n",
      "+ pi_0: the probability of locating in the first cluster\n",
      "+ pi_1: the probability of locating in the second cluster\n",
      "+ init_index_0: the index of data that are initialized in the first cluster\n",
      "+ init_index_1: the index of data that are initialized in the second cluster\n",
      "+ clust0_v: data that are initialized in the first cluster\n",
      "+ clust1_v: data that are initialized in the second cluster"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Implement the integration:\n",
      "\n",
      "$$P(\\mathcal{D}|H_1)= \\int f(x|\\mu)f(\\mu) \\mathcal{d}\\mu = \\int_{-\\infty}^{\\infty} \\bigg[\\frac{1}{\\sigma_0 \\sqrt{2\\pi}}exp(-\\frac{(\\mu-\\mu_0)^2}{2\\sigma_0^2})\\bigg]\\bigg[(\\frac{1}{\\sigma \\sqrt{s\\pi}})^n exp(-\\frac{n(\\bar{x}-\\mu)^2}{2\\sigma^2})   \\bigg]\\mathcal{d}\\mu$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.integrate import quad\n",
      "\n",
      "def mu_int(sigma_null, mu_null, sigma, N, X):\n",
      "    k1 = 1.0 / (sigma_null * math.sqrt(2*math.pi))\n",
      "    s1 = -1.0 / (2 * sigma_null * sigma_null)\n",
      "    \n",
      "    k2 = 1.0 / (sigma * math.sqrt(2*math.pi))\n",
      "    s2 = -1.0 / (2 * sigma * sigma)\n",
      "    \n",
      "    x_bar = X*1.0/N\n",
      "    \n",
      "    def f(mu):\n",
      "        return (k1 * math.exp(s1 * (mu - mu_null)*(mu - mu_null)))*(k2 * math.exp(s2 * N*(mu - x_bar)*(mu - x_bar)))\n",
      "    return f\n",
      "\n",
      "#quad(mu_int(10, 10, 10, 10, 105), -inf, inf)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 73
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data[2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 74,
       "text": [
        "array([ 1.28000783])"
       ]
      }
     ],
     "prompt_number": 74
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# initialize the first two pairs of points\n",
      "init_index_0 = np.array([1,3])\n",
      "init_index_1 = np.array([2,4])\n",
      "\n",
      "clust0_v = np.array([data[1], data[3]])\n",
      "clust1_v = np.array([data[2], data[4]])\n",
      "\n",
      "clust0_v\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 75,
       "text": [
        "array([[ 20.26957114],\n",
        "       [ 19.7095942 ]])"
       ]
      }
     ],
     "prompt_number": 75
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sigma_null = 1\n",
      "mu_null = 1\n",
      "sigma = 1\n",
      "N0 = clust0_v.size\n",
      "sum0 = sum(clust0_v)\n",
      "\n",
      "quad(mu_int(sigma_null, mu_null, sigma, N0, sum0), -inf, inf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 76,
       "text": [
        "(3.0945688146770054e-53, 5.753876438155679e-53)"
       ]
      }
     ],
     "prompt_number": 76
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import operator\n",
      "\n",
      "# initialize the first two pairs of points\n",
      "init_index_0 = np.array([1,3])\n",
      "init_index_1 = np.array([2,4])\n",
      "\n",
      "clust0_v = np.array([data[1], data[3]])\n",
      "clust1_v = np.array([data[2], data[4]])\n",
      "\n",
      "def hcluster(df, mu_0, mu_1, sigma, mu_null, sigma_null, pi, init_index_0, init_index_1, clust0_v, clust1_v):\n",
      "    closed = 0.0\n",
      "    # initialize the T_i for clust 0: 2 initial selected values\n",
      "    N0 = clust0_v.size\n",
      "    sumclust0 = sum(clust0_v)\n",
      "    t_clust_0 = quad(mu_int(sigma_null, mu_null, sigma, N0, sumclust0), -inf, inf)[1]\n",
      "    \n",
      "    # initialize the T_j for clust 1: 2 initial selected values\n",
      "    N1 = clust1_v.size\n",
      "    sumclust1 = sum(clust1_v)\n",
      "    t_clust_1 = quad(mu_int(sigma_null, mu_null, sigma, N1, sumclust1), -inf, inf)[1]\n",
      "    \n",
      "    n = df.shape[0]\n",
      "    # go through all the points\n",
      "    c = 10\n",
      "    while c > 0:\n",
      "        for j in range(n):\n",
      "\n",
      "            closest_lik = 0.0\n",
      "            clust_0 = dict()\n",
      "            clust_1 = dict()\n",
      "            for i in range(n):            \n",
      "                # traverse of the left array\n",
      "                # i must not exist in the already exist init_index\n",
      "                if (i not in init_index_0 and i not in init_index_1):\n",
      "                    tempx_clust0_v = np.append([clust0_v], [df.ix[i,0]])\n",
      "                    tempx_clust1_v = np.append([clust1_v], [df.ix[i,0]])\n",
      "                \n",
      "                    # likeli for null under clust 0\n",
      "                    X0 = sum(tempx_clust0_v)\n",
      "                    N0 = tempx_clust0_v.size\n",
      "                    lik_alt_0 = pi*quad(mu_int(sigma_null, mu_null, sigma, N0, X0), -inf, inf)[0]\n",
      "                \n",
      "                    # likeli for alternative under clust 0\n",
      "                    X1 = sum(tempx_clust0_v[:N0-1])\n",
      "                    N1 = tempx_clust1_v.size\n",
      "                    X2 = sum(tempx_clust1_v)\n",
      "                    D_i = quad(mu_int(sigma_null, mu_null, sigma, N0-1, X1), -inf, inf)[0]\n",
      "                    D_j = quad(mu_int(sigma_null, mu_null, sigma, N1, X2), -inf, inf)[0]\n",
      "                    lik_alt_1 = (1 - pi)*D_i*D_j\n",
      "                    # the ratio under the 1st cluster\n",
      "                    r_0 = lik_alt_0/(lik_alt_0+lik_alt_1)\n",
      "                \n",
      "                    # likeli for null under cluster 1\n",
      "                    X0_1 = sum(tempx_clust1_v)\n",
      "                    N0_1 = tempx_clust1_v.size\n",
      "                    lik_alt_0_1 = pi*quad(mu_int(sigma_null, mu_null, sigma, N0_1, X0_1), -inf, inf)[0]\n",
      "           \n",
      "                    # likeli for alternative under cluster 0\n",
      "                    X1_1 = sum(tempx_clust1_v[:N0_1-1])\n",
      "                    N1_1 = tempx_clust0_v.size\n",
      "                    X2_1 = sum(tempx_clust0_v)\n",
      "                    D_i_1 = quad(mu_int(sigma_null, mu_null, sigma, N0_1-1, X1_1), -inf, inf)[0]\n",
      "                    D_j_1 = quad(mu_int(sigma_null, mu_null, sigma, N1_1, X2_1), -inf, inf)[0]\n",
      "                    lik_alt_1_1 = (1-pi) * D_i_1 * D_j_1\n",
      "                    # the ratio under the 2nd cluster \n",
      "                    r_0_1 = lik_alt_0_1/(lik_alt_0_1+lik_alt_1_1)\n",
      "                \n",
      "                    if r_0 < r_0_1:\n",
      "                        post = lik_alt_0\n",
      "                        clust_0.update({i:post})\n",
      "                    else:\n",
      "                        post1 = lik_alt_0_1\n",
      "                        clust_1.update({i:post1})\n",
      "      \n",
      "\n",
      "\n",
      "            # select the max likelihood from the two lists\n",
      "            if (bool(clust_0) == True):\n",
      "                key_0,maxLik_0 = max(clust_0.iteritems(), key=lambda x:x[1])\n",
      "                clust0_v = np.append(clust0_v,df.ix[key_0,0])\n",
      "                init_index_0 = np.append(init_index_0,key_0)\n",
      "            if (bool(clust_1) == True):\n",
      "                key_1,maxLik_1 = max(clust_1.iteritems(), key=lambda x:x[1])\n",
      "                clust1_v = np.append(clust1_v,df.ix[key_1,0])\n",
      "                init_index_1 = np.append(init_index_1,key_1)\n",
      "\n",
      "            #clust0_v = np.append(clust0_v,df.ix[key_0,0])\n",
      "            #clust1_v = np.append(clust1_v,df.ix[key_1,0])\n",
      "        \n",
      "            #init_index_0 = np.append(init_index_0,key_0)\n",
      "            #init_index_1 = np.append(init_index_1,key_1)\n",
      "            c = c-1\n",
      "            print init_index_0, init_index_1\n",
      "\n",
      "hcluster(df, mu_0 = 1, mu_1 = 20, sigma = 1, mu_null=10, sigma_null=1, pi=0.5, init_index_0 = init_index_0, init_index_1 = init_index_1, clust0_v = clust0_v, clust1_v = clust1_v)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1 3 7] [ 2  4 18]\n",
        "[ 1  3  7 15] [ 2  4 18  0]\n",
        "[ 1  3  7 15  9]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [ 2  4 18  0  6]\n",
        "[ 1  3  7 15  9 13] [ 2  4 18  0  6  8]\n",
        "[ 1  3  7 15  9 13 19] [ 2  4 18  0  6  8 16]\n",
        "[ 1  3  7 15  9 13 19 11] [ 2  4 18  0  6  8 16 12]\n",
        "[ 1  3  7 15  9 13 19 11  5] [ 2  4 18  0  6  8 16 12 14]\n",
        "[ 1  3  7 15  9 13 19 11  5 17] [ 2  4 18  0  6  8 16 12 14 10]\n",
        "[ 1  3  7 15  9 13 19 11  5 17] [ 2  4 18  0  6  8 16 12 14 10]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[ 1  3  7 15  9 13 19 11  5 17] [ 2  4 18  0  6  8 16 12 14 10]\n",
        "[ 1  3  7 15  9 13 19 11  5 17] [ 2  4 18  0  6  8 16 12 14 10]\n",
        "[ 1  3  7 15  9 13 19 11  5 17] [ 2  4 18  0  6  8 16 12 14 10]\n",
        "[ 1  3  7 15  9 13 19 11  5 17] [ 2  4 18  0  6  8 16 12 14 10]\n",
        "[ 1  3  7 15  9 13 19 11  5 17] [ 2  4 18  0  6  8 16 12 14 10]\n",
        "[ 1  3  7 15  9 13 19 11  5 17] [ 2  4 18  0  6  8 16 12 14 10]\n",
        "[ 1  3  7 15  9 13 19 11  5 17] [ 2  4 18  0  6  8 16 12 14 10]\n",
        "[ 1  3  7 15  9 13 19 11  5 17] [ 2  4 18  0  6  8 16 12 14 10]\n",
        "[ 1  3  7 15  9 13 19 11  5 17] [ 2  4 18  0  6  8 16 12 14 10]\n",
        "[ 1  3  7 15  9 13 19 11  5 17] [ 2  4 18  0  6  8 16 12 14 10]\n",
        "[ 1  3  7 15  9 13 19 11  5 17] [ 2  4 18  0  6  8 16 12 14 10]\n"
       ]
      }
     ],
     "prompt_number": 79
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n=clust0_v.size\n",
      "clust0_v[:n-1]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "array([[ 21.61254932]])"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}